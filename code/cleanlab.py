{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Centric NLP 대회: 주제 분류 프로젝트"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.26.0 in /opt/conda/lib/python3.8/site-packages (4.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.26.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (3.7.4.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.26.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.26.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.26.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.26.0) (2020.12.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /opt/conda/lib/python3.8/site-packages (0.1.96)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (1.23.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: evaluate==0.4.0 in /opt/conda/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (1.23.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (2.30.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (0.20.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from evaluate==0.4.0) (0.18.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.0.12)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (0.6)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->evaluate==0.4.0) (2020.12.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas->evaluate==0.4.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->evaluate==0.4.0) (2020.5)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas->evaluate==0.4.0) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.0) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.8/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate) (5.7.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate) (5.3.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from accelerate) (1.11.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.8/site-packages (from accelerate) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub->accelerate) (3.0.12)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.30.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2020.12.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.23.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.6)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.23.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.7.2)\n",
      "Requirement already satisfied: pyzmq>=20 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.7.4)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.7.4.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: protobuf==3.15.8 in /opt/conda/lib/python3.8/site-packages (3.15.8)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.8/site-packages (from protobuf==3.15.8) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.11.0) (3.7.4.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# package install \n",
    "!pip install transformers==4.26.0\n",
    "!pip install sentencepiece==0.1.96\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install evaluate==0.4.0\n",
    "!pip install accelerate\n",
    "!pip install scikit-learn\n",
    "!pip install ipywidgets\n",
    "!pip install protobuf==3.15.8\n",
    "!pip install torch==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 456\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '../data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, '../output')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'klue/bert-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_DIR, 'before_cleanlab_train.csv'))\n",
    "dataset_train, dataset_valid = train_test_split(data, test_size=0.3, stratify=data['target'],random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>겨울에도 수영장이 따뜻하네3400억 투자금 유치 컴퓨터로 데웠다테크토크</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5G 28 주파수 값 2000억 육박예상보다 많이 올랐다종합</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>컴퓨터비전 분야 AI 인간보다 경제성 떨어진다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>獨 철도 30년 된 MSDOS윈도 3.11 관리할 경력자 찾아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>인류 첫 지구 아닌 행성 비행 화성탐사 인저뉴이티 역사속으로</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16928</th>\n",
       "      <td>힐만 SK 감독 고통스럽지만 내 상황 솔직히 알려야 해</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16929</th>\n",
       "      <td>정의장 사드 국회동의 사안 아니라 쳐도 충분히 협의해야</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16930</th>\n",
       "      <td>정치권 엘시티 수사 돌발변수에 촉각…왜 지금</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16931</th>\n",
       "      <td>문 대통령 1987 관람…깜짝 방문에 객석 환호·박수종합</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16932</th>\n",
       "      <td>120년 전 대한제국으로…가을밤 정동에서 시간 여행 떠나다</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16933 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text  target\n",
       "0      겨울에도 수영장이 따뜻하네3400억 투자금 유치 컴퓨터로 데웠다테크토크       0\n",
       "1            5G 28 주파수 값 2000억 육박예상보다 많이 올랐다종합       0\n",
       "2                    컴퓨터비전 분야 AI 인간보다 경제성 떨어진다       0\n",
       "3           獨 철도 30년 된 MSDOS윈도 3.11 관리할 경력자 찾아       0\n",
       "4            인류 첫 지구 아닌 행성 비행 화성탐사 인저뉴이티 역사속으로       0\n",
       "...                                        ...     ...\n",
       "16928           힐만 SK 감독 고통스럽지만 내 상황 솔직히 알려야 해       5\n",
       "16929           정의장 사드 국회동의 사안 아니라 쳐도 충분히 협의해야       6\n",
       "16930                 정치권 엘시티 수사 돌발변수에 촉각…왜 지금       6\n",
       "16931          문 대통령 1987 관람…깜짝 방문에 객석 환호·박수종합       6\n",
       "16932         120년 전 대한제국으로…가을밤 정동에서 시간 여행 떠나다       3\n",
       "\n",
       "[16933 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        input_texts = data['text']\n",
    "        targets = data['target']\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for text, label in zip(input_texts, targets):\n",
    "            tokenized_input = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "            self.inputs.append(tokenized_input)\n",
    "            self.labels.append(torch.tensor(label))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx]['input_ids'].squeeze(0),  \n",
    "            'attention_mask': self.inputs[idx]['attention_mask'].squeeze(0),\n",
    "            'labels': self.labels[idx].squeeze(0)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = BERTDataset(dataset_train, tokenizer)\n",
    "data_valid = BERTDataset(dataset_valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for wandb setting\n",
    "#os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate= 2e-05,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_valid,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11853\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5928\n",
      "  Number of trainable parameters = 110622727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5928' max='5928' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5928/5928 1:13:52, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.021900</td>\n",
       "      <td>0.817244</td>\n",
       "      <td>0.779349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.829400</td>\n",
       "      <td>0.804415</td>\n",
       "      <td>0.789576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.821200</td>\n",
       "      <td>0.708378</td>\n",
       "      <td>0.811569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.712300</td>\n",
       "      <td>0.783747</td>\n",
       "      <td>0.806309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.831900</td>\n",
       "      <td>0.685973</td>\n",
       "      <td>0.816293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.747100</td>\n",
       "      <td>0.809211</td>\n",
       "      <td>0.810328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.814500</td>\n",
       "      <td>0.804722</td>\n",
       "      <td>0.814399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.833700</td>\n",
       "      <td>0.654514</td>\n",
       "      <td>0.822757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.735500</td>\n",
       "      <td>0.774906</td>\n",
       "      <td>0.821983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.678900</td>\n",
       "      <td>0.753278</td>\n",
       "      <td>0.829419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>0.691547</td>\n",
       "      <td>0.828875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.736500</td>\n",
       "      <td>0.686769</td>\n",
       "      <td>0.827845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.690200</td>\n",
       "      <td>0.787229</td>\n",
       "      <td>0.821367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.733300</td>\n",
       "      <td>0.733871</td>\n",
       "      <td>0.824503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.605538</td>\n",
       "      <td>0.833476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>0.693292</td>\n",
       "      <td>0.828648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.634700</td>\n",
       "      <td>0.686802</td>\n",
       "      <td>0.832501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.625312</td>\n",
       "      <td>0.836891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.623676</td>\n",
       "      <td>0.841724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>0.638200</td>\n",
       "      <td>0.845523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.612028</td>\n",
       "      <td>0.841744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.747141</td>\n",
       "      <td>0.828724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.688466</td>\n",
       "      <td>0.837824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.620500</td>\n",
       "      <td>0.705912</td>\n",
       "      <td>0.839529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.791400</td>\n",
       "      <td>0.657762</td>\n",
       "      <td>0.840446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.654197</td>\n",
       "      <td>0.837123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.712300</td>\n",
       "      <td>0.679583</td>\n",
       "      <td>0.830942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.600670</td>\n",
       "      <td>0.843028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.564300</td>\n",
       "      <td>0.638315</td>\n",
       "      <td>0.839816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.637066</td>\n",
       "      <td>0.850521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>0.725415</td>\n",
       "      <td>0.842333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.753139</td>\n",
       "      <td>0.844695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.351800</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.843315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.520500</td>\n",
       "      <td>0.718824</td>\n",
       "      <td>0.848287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.764119</td>\n",
       "      <td>0.840793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.393300</td>\n",
       "      <td>0.755663</td>\n",
       "      <td>0.849847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.382200</td>\n",
       "      <td>0.763758</td>\n",
       "      <td>0.849044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.795318</td>\n",
       "      <td>0.845533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>0.763330</td>\n",
       "      <td>0.850161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.757071</td>\n",
       "      <td>0.846972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.727317</td>\n",
       "      <td>0.851986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.723624</td>\n",
       "      <td>0.853110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.622800</td>\n",
       "      <td>0.709072</td>\n",
       "      <td>0.849530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>0.739008</td>\n",
       "      <td>0.852225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.720136</td>\n",
       "      <td>0.851808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.715815</td>\n",
       "      <td>0.854627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>0.726362</td>\n",
       "      <td>0.855419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.742114</td>\n",
       "      <td>0.853379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.750567</td>\n",
       "      <td>0.853704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.753338</td>\n",
       "      <td>0.852856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.756944</td>\n",
       "      <td>0.852771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.748964</td>\n",
       "      <td>0.853812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.728438</td>\n",
       "      <td>0.855066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.325400</td>\n",
       "      <td>0.725954</td>\n",
       "      <td>0.855392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.721656</td>\n",
       "      <td>0.854318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.459800</td>\n",
       "      <td>0.723236</td>\n",
       "      <td>0.854111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.725168</td>\n",
       "      <td>0.855066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.723736</td>\n",
       "      <td>0.855525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.723060</td>\n",
       "      <td>0.855394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-100\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-100/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-100/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-200\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-200/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-300\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-300/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-400\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-400/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-500\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-500/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-600\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-600/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-700\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-700/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-800\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-800/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-900\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-900/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1000\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1000/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1100\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1100/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1200\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1200/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1300\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1300/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1400\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1400/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1500\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1500/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1600\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1600/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1700\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1700/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1800\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1800/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-1900\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-1900/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-1900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2000\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2000/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2100\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2100/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2100/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2200\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2200/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2200/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2300\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2300/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2400\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2400/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2500\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2500/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2600\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2600/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2700\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2700/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2800\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2800/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-2900\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-2900/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-2900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3000\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3000/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3100\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3100/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3100/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-2900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3200\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3200/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3200/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3300\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3300/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3400\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3400/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3500\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3500/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3600\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3600/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3700\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3700/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3800\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3800/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-3900\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-3900/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-3900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4000\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4000/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4100\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4100/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4100/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4200\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4200/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4200/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4300\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4300/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4400\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4400/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4500\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4500/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4600\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4600/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4700\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4700/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4800\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4800/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-4900\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-4900/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-4900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5000\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5000/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5100\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5100/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5100/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5200\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5200/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5200/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5300\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5300/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5300/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5400\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5400/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5400/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5500\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5500/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5600\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5600/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5600/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5700\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5700/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5700/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5800\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5800/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5800/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-4700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5080\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /data/ephemeral/code/../output/checkpoint-5900\n",
      "Configuration saved in /data/ephemeral/code/../output/checkpoint-5900/config.json\n",
      "Model weights saved in /data/ephemeral/code/../output/checkpoint-5900/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/ephemeral/code/../output/checkpoint-5700] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /data/ephemeral/code/../output/checkpoint-5800 (score: 0.8555254023312289).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5928, training_loss=0.5718842012840405, metrics={'train_runtime': 4432.8554, 'train_samples_per_second': 5.348, 'train_steps_per_second': 1.337, 'total_flos': 6237590689843200.0, 'train_loss': 0.5718842012840405, 'epoch': 2.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47785it [05:55, 134.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# model.eval()\n",
    "# preds = []\n",
    "# for idx, sample in tqdm(dataset_test.iterrows()):\n",
    "#     inputs = tokenizer(sample['text'], return_tensors=\"pt\").to(DEVICE)\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(**inputs).logits\n",
    "#         pred = torch.argmax(torch.nn.Softmax(dim=1)(logits), dim=1).cpu().numpy()\n",
    "#         preds.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test['target'] = preds\n",
    "# dataset_test.to_csv(os.path.join(BASE_DIR, 'output.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_dev_00000</td>\n",
       "      <td>제임스 부상 레이커스 성탄매치서 골든스테이트에 완승종합</td>\n",
       "      <td>https://sports.news.naver.com/news.nhn?oid=001...</td>\n",
       "      <td>2018.12.26 15:16</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_dev_00001</td>\n",
       "      <td>프랑스 극우정치인 르펜 노란 조끼 덕에 승승장구</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D...</td>\n",
       "      <td>2019.01.17. 오후 8:04</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_dev_00002</td>\n",
       "      <td>대통령개헌안 ⑥토지공개념 명시…개발이익환수·부동산과세 강화 전망종합</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D...</td>\n",
       "      <td>2018.03.21. 오후 1:09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_dev_00003</td>\n",
       "      <td>의사 살해 환자 경찰서 나와 법원으로</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D...</td>\n",
       "      <td>2019.01.02. 오후 2:47</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_dev_00004</td>\n",
       "      <td>이란 최고지도자 유럽 맹비난…핵합의 미이행 뻔뻔하고 오만</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D...</td>\n",
       "      <td>2019.07.16. 오후 8:11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                                   text   \n",
       "0  ynat-v1_dev_00000         제임스 부상 레이커스 성탄매치서 골든스테이트에 완승종합  \\\n",
       "1  ynat-v1_dev_00001             프랑스 극우정치인 르펜 노란 조끼 덕에 승승장구   \n",
       "2  ynat-v1_dev_00002  대통령개헌안 ⑥토지공개념 명시…개발이익환수·부동산과세 강화 전망종합   \n",
       "3  ynat-v1_dev_00003                   의사 살해 환자 경찰서 나와 법원으로   \n",
       "4  ynat-v1_dev_00004        이란 최고지도자 유럽 맹비난…핵합의 미이행 뻔뻔하고 오만   \n",
       "\n",
       "                                                 url                 date   \n",
       "0  https://sports.news.naver.com/news.nhn?oid=001...     2018.12.26 15:16  \\\n",
       "1  https://news.naver.com/main/read.nhn?mode=LS2D...  2019.01.17. 오후 8:04   \n",
       "2  https://news.naver.com/main/read.nhn?mode=LS2D...  2018.03.21. 오후 1:09   \n",
       "3  https://news.naver.com/main/read.nhn?mode=LS2D...  2019.01.02. 오후 2:47   \n",
       "4  https://news.naver.com/main/read.nhn?mode=LS2D...  2019.07.16. 오후 8:11   \n",
       "\n",
       "   target  \n",
       "0       5  \n",
       "1       4  \n",
       "2       1  \n",
       "3       2  \n",
       "4       4  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting cleanlab\n",
      "  Downloading cleanlab-2.5.0-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.8/site-packages (from cleanlab) (1.23.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /opt/conda/lib/python3.8/site-packages (from cleanlab) (1.2.2)\n",
      "Requirement already satisfied: tqdm>=4.53.0 in /opt/conda/lib/python3.8/site-packages (from cleanlab) (4.66.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.8/site-packages (from cleanlab) (2.0.1)\n",
      "Collecting termcolor>=2.0.0 (from cleanlab)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.5->cleanlab) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.5->cleanlab) (2020.5)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.1.5->cleanlab) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0->cleanlab) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0->cleanlab) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=1.0->cleanlab) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->cleanlab) (1.15.0)\n",
      "Downloading cleanlab-2.5.0-py3-none-any.whl (285 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: termcolor, cleanlab\n",
      "Successfully installed cleanlab-2.5.0 termcolor-2.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16933it [02:10, 129.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "for idx, sample in tqdm(data.iterrows()):\n",
    "    inputs = tokenizer(sample['text'], return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.nn.Softmax(dim=1)(logits).cpu().numpy()\n",
    "        preds.append(probs[0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2topic = {0: 'IT과학', 1: '경제', 2: '사회', 3: '생활문화', 4: '세계', 5: '스포츠', 6: '정치'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "16933\n",
      "987\n",
      "[10, 22, 38, 50, 54, 56, 67, 71, 72, 84, 118, 123, 124, 127, 130, 137, 156, 159, 168, 170, 176, 199, 225, 228, 231, 232, 233, 241, 252, 256, 264, 267, 292, 308, 314, 327, 330, 331, 363, 385, 388, 400, 406, 407, 409, 413, 423, 425, 433, 434, 445, 468, 477, 483, 487, 490, 497, 501, 523, 563, 576, 578, 605, 612, 618, 628, 630, 633, 638, 650, 653, 661, 663, 672, 678, 679, 691, 696, 712, 715, 722, 734, 736, 742, 750, 752, 756, 757, 769, 783, 784, 787, 798, 812, 816, 823, 834, 842, 860, 863, 867, 869, 876, 904, 905, 906, 920, 978, 980, 994, 995, 1032, 1038, 1041, 1043, 1046, 1078, 1107, 1111, 1122, 1141, 1146, 1166, 1169, 1181, 1214, 1224, 1250, 1275, 1284, 1313, 1332, 1341, 1351, 1357, 1359, 1360, 1368, 1384, 1386, 1395, 1420, 1446, 1447, 1458, 1459, 1460, 1467, 1474, 1481, 1488, 1489, 1497, 1512, 1522, 1526, 1527, 1553, 1556, 1566, 1596, 1604, 1615, 1632, 1635, 1644, 1648, 1660, 1662, 1680, 1682, 1685, 1689, 1690, 1704, 1738, 1741, 1750, 1763, 1769, 1772, 1775, 1778, 1800, 1807, 1811, 1824, 1828, 1831, 1846, 1854, 1855, 1856, 1867, 1877, 1889, 1895, 1901, 1931, 1940, 1973, 1986, 2000, 2008, 2020, 2037, 2046, 2051, 2054, 2057, 2059, 2076, 2079, 2086, 2094, 2095, 2118, 2127, 2143, 2152, 2156, 2161, 2162, 2163, 2170, 2171, 2173, 2185, 2221, 2225, 2227, 2232, 2267, 2276, 2277, 2287, 2300, 2302, 2303, 2312, 2316, 2356, 2370, 2371, 2380, 2398, 2402, 2406, 2431, 2432, 2443, 2445, 2458, 2459, 2486, 2499, 2500, 2520, 2521, 2530, 2537, 2550, 2591, 2592, 2593, 2600, 2602, 2606, 2628, 2670, 2673, 2682, 2685, 2693, 2708, 2720, 2726, 2732, 2743, 2765, 2767, 2773, 2776, 2787, 2803, 2818, 2821, 2830, 2847, 2855, 2873, 2877, 2893, 2911, 2916, 2927, 2929, 2933, 2948, 2949, 2965, 2966, 2967, 2969, 2975, 2977, 2993, 2995, 2996, 2997, 3009, 3015, 3033, 3037, 3042, 3062, 3065, 3069, 3093, 3094, 3114, 3119, 3127, 3129, 3154, 3187, 3188, 3194, 3213, 3234, 3245, 3247, 3306, 3309, 3327, 3330, 3337, 3351, 3392, 3406, 3411, 3413, 3415, 3428, 3429, 3434, 3445, 3448, 3456, 3478, 3479, 3516, 3525, 3552, 3563, 3565, 3583, 3605, 3613, 3614, 3640, 3667, 3672, 3712, 3713, 3715, 3725, 3738, 3744, 3762, 3770, 3783, 3786, 3798, 3813, 3819, 3829, 3833, 3846, 3882, 3921, 3927, 3930, 3939, 3959, 3972, 3998, 4006, 4039, 4060, 4065, 4066, 4072, 4077, 4096, 4099, 4100, 4101, 4107, 4130, 4156, 4160, 4161, 4171, 4183, 4193, 4219, 4220, 4235, 4236, 4278, 4287, 4292, 4312, 4318, 4319, 4336, 4348, 4351, 4390, 4412, 4437, 4452, 4455, 4464, 4481, 4501, 4525, 4530, 4545, 4555, 4575, 4581, 4586, 4612, 4614, 4622, 4638, 4652, 4657, 4659, 4677, 4683, 4685, 4702, 4703, 4711, 4716, 4720, 4732, 4744, 4745, 4756, 4767, 4773, 4790, 4817, 4820, 4827, 4835, 4843, 4844, 4853, 4873, 4880, 4882, 4889, 4934, 4942, 4951, 4955, 4964, 4965, 4980, 5001, 5003, 5027, 5034, 5052, 5061, 5064, 5083, 5093, 5094, 5096, 5116, 5127, 5141, 5143, 5169, 5171, 5173, 5189, 5193, 5216, 5221, 5228, 5229, 5233, 5296, 5308, 5309, 5331, 5332, 5372, 5379, 5407, 5413, 5423, 5425, 5426, 5430, 5448, 5464, 5467, 5476, 5518, 5525, 5572, 5583, 5604, 5616, 5624, 5638, 5643, 5657, 5660, 5661, 5669, 5689, 5717, 5720, 5721, 5728, 5736, 5737, 5738, 5751, 5759, 5761, 5774, 5775, 5780, 5786, 5795, 5807, 5832, 5836, 5842, 5853, 5859, 5861, 5877, 5881, 5883, 5900, 5916, 5925, 5927, 5932, 5936, 5939, 5945, 5961, 5974, 5989, 6010, 6017, 6019, 6022, 6033, 6045, 6076, 6085, 6089, 6094, 6114, 6156, 6173, 6179, 6200, 6203, 6222, 6223, 6235, 6258, 6264, 6265, 6270, 6277, 6286, 6296, 6308, 6322, 6344, 6366, 6376, 6378, 6384, 6407, 6413, 6429, 6455, 6467, 6491, 6531, 6533, 6549, 6551, 6582, 6598, 6600, 6608, 6610, 6612, 6631, 6636, 6641, 6655, 6681, 6714, 6720, 6754, 6790, 6792, 6819, 6837, 6845, 6855, 6859, 6860, 6885, 6889, 6929, 6932, 6937, 6942, 6961, 6982, 7016, 7018, 7028, 7035, 7037, 7051, 7056, 7080, 7084, 7122, 7129, 7144, 7153, 7193, 7204, 7224, 7266, 7274, 7278, 7298, 7326, 7358, 7406, 7428, 7445, 7468, 7512, 7521, 7525, 7535, 7601, 7614, 7648, 7654, 7659, 7677, 7694, 7704, 7722, 7750, 7786, 7792, 7796, 7807, 7832, 7841, 7889, 7894, 7918, 7940, 7952, 7999, 8002, 8011, 8021, 8070, 8100, 8108, 8148, 8198, 8212, 8223, 8224, 8268, 8297, 8343, 8408, 8415, 8427, 8471, 8527, 8535, 8551, 8576, 8612, 8613, 8634, 8656, 8692, 8771, 8813, 8822, 8842, 8860, 8871, 8874, 8995, 9011, 9025, 9034, 9041, 9105, 9112, 9183, 9222, 9252, 9270, 9277, 9287, 9308, 9311, 9319, 9329, 9445, 9461, 9521, 9534, 9561, 9567, 9577, 9686, 9700, 9746, 9750, 9767, 9796, 9799, 9843, 9854, 9858, 9866, 9868, 9904, 9936, 9939, 9954, 9980, 9992, 10052, 10062, 10063, 10090, 10168, 10185, 10189, 10214, 10219, 10221, 10230, 10236, 10277, 10298, 10313, 10323, 10385, 10439, 10617, 10628, 10636, 10647, 10690, 10718, 10737, 10750, 10752, 10759, 10782, 10809, 10922, 10933, 10943, 10965, 11002, 11085, 11133, 11153, 11155, 11196, 11273, 11382, 11448, 11451, 11492, 11560, 11581, 11586, 11595, 11617, 11683, 11706, 11731, 11750, 11764, 11836, 11852, 11914, 11924, 11951, 12035, 12037, 12066, 12086, 12159, 12193, 12199, 12214, 12216, 12230, 12316, 12356, 12361, 12391, 12402, 12429, 12436, 12454, 12459, 12629, 12664, 12730, 12742, 12819, 12827, 12849, 12944, 12967, 13031, 13048, 13052, 13071, 13083, 13091, 13125, 13146, 13147, 13244, 13276, 13387, 13422, 13451, 13485, 13512, 13522, 13535, 13562, 13589, 13598, 13603, 13629, 13652, 13707, 13717, 13725, 13729, 13770, 13781, 13791, 13797, 13812, 13856, 13919, 13928, 13931, 13948, 13952, 13977, 13994, 13997, 14032, 14043, 14052, 14087, 14099, 14142, 14204, 14322, 14331, 14340, 14344, 14367, 14368, 14383, 14389, 14398, 14457, 14467, 14480, 14513, 14602, 14631, 14639, 14642, 14645, 14805, 14841, 14850, 14852, 14860, 14919, 14961, 15006, 15078, 15101, 15128, 15160, 15165, 15189, 15205, 15337, 15346, 15347, 15370, 15384, 15415, 15464, 15520, 15525, 15526, 15547, 15571, 15643, 15655, 15707, 15769, 15772, 15776, 15779, 15790, 15791, 15842, 15848, 15872, 15875, 15906, 15956, 15961, 15978, 15988, 16031, 16054, 16067, 16158, 16239, 16276, 16287, 16301, 16375, 16381, 16437, 16550, 16564, 16612, 16627, 16628, 16632, 16634, 16683, 16912]\n",
      "input text: 윤 대통령 박상욱 초대 과기수석 내정RD 논란 등 과제 산적\n",
      "label: IT과학\n",
      "-----------\n",
      "input text: 70대 푸틴 영하 5도에 얼음물 입수함부로 풍덩 했다간\n",
      "label: 생활문화\n",
      "-----------\n",
      "input text: 연합뉴스 이 시각 헤드라인  1030\n",
      "label: 경제\n",
      "-----------\n",
      "input text: 프로골퍼 박민지 대한항공 항공권 공짜로 받는 사연은\n",
      "label: 경제\n",
      "-----------\n",
      "input text: 與 전국 주요 도시 철도지하화광역 급행열차 1시간 생활권 조성\n",
      "label: 경제\n",
      "-----------\n",
      "input text: 中 징둥 수소 물류트럭 첫 대규모 상용화\n",
      "label: 생활문화\n",
      "-----------\n",
      "input text: 사무실 안 나오면 징계 초강수1년 전과 180도 달라졌다\n",
      "label: 세계\n",
      "-----------\n",
      "input text: 미국은 한국에 美軍 주둔 대가 원한다\n",
      "label: 생활문화\n",
      "-----------\n",
      "input text: 명동에 퍼진 임윤찬 피아노 연주클래식 잔잔함 속에 충격과 희망 주죠\n",
      "label: IT과학\n",
      "-----------\n",
      "input text: 윤정호의 오늘 찐명 타령\n",
      "label: 정치\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "from cleanlab.filter import find_label_issues\n",
    "import numpy as np\n",
    "pred_probs_array = np.array(preds)\n",
    "ordered_label_issues = find_label_issues(\n",
    "    labels=data['target'], #데이터셋 라벨\n",
    "    pred_probs=pred_probs_array, #정답 예측 확률\n",
    "    return_indices_ranked_by='self_confidence',\n",
    ")\n",
    "print(len(data))\n",
    "print(len(ordered_label_issues))\n",
    "print(sorted(ordered_label_issues))\n",
    "head_issues=ordered_label_issues[:10]\n",
    "for issue in head_issues:\n",
    "    print('input text:',data.iloc[issue]['text'])\n",
    "    print('label:',num2topic[data.iloc[issue]['target']])\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "|  Generating a Cleanlab Dataset Health Summary           |\n",
      "|   for your dataset with 16,933 examples and 7 classes.  |\n",
      "|  Note, Cleanlab is not a medical doctor... yet.         |\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Overall Class Quality and Noise across your dataset (below)\n",
      "------------------------------------------------------------ \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Label Issues</th>\n",
       "      <th>Inverse Label Issues</th>\n",
       "      <th>Label Noise</th>\n",
       "      <th>Inverse Label Noise</th>\n",
       "      <th>Label Quality Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>사회</td>\n",
       "      <td>2</td>\n",
       "      <td>234</td>\n",
       "      <td>170</td>\n",
       "      <td>0.085153</td>\n",
       "      <td>0.063338</td>\n",
       "      <td>0.914847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경제</td>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>194</td>\n",
       "      <td>0.082237</td>\n",
       "      <td>0.071719</td>\n",
       "      <td>0.917763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT과학</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>216</td>\n",
       "      <td>0.066937</td>\n",
       "      <td>0.085851</td>\n",
       "      <td>0.933063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>생활문화</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>164</td>\n",
       "      <td>0.059794</td>\n",
       "      <td>0.067103</td>\n",
       "      <td>0.940206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정치</td>\n",
       "      <td>6</td>\n",
       "      <td>103</td>\n",
       "      <td>82</td>\n",
       "      <td>0.037646</td>\n",
       "      <td>0.030203</td>\n",
       "      <td>0.962354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>세계</td>\n",
       "      <td>4</td>\n",
       "      <td>105</td>\n",
       "      <td>144</td>\n",
       "      <td>0.036881</td>\n",
       "      <td>0.049896</td>\n",
       "      <td>0.963119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>스포츠</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>0.017294</td>\n",
       "      <td>0.989754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class Name  Class Index  Label Issues  Inverse Label Issues  Label Noise   \n",
       "0         사회            2           234                   170     0.085153  \\\n",
       "1         경제            1           225                   194     0.082237   \n",
       "2       IT과학            0           165                   216     0.066937   \n",
       "3       생활문화            3           145                   164     0.059794   \n",
       "4         정치            6           103                    82     0.037646   \n",
       "5         세계            4           105                   144     0.036881   \n",
       "6        스포츠            5            10                    17     0.010246   \n",
       "\n",
       "   Inverse Label Noise  Label Quality Score  \n",
       "0             0.063338             0.914847  \n",
       "1             0.071719             0.917763  \n",
       "2             0.085851             0.933063  \n",
       "3             0.067103             0.940206  \n",
       "4             0.030203             0.962354  \n",
       "5             0.049896             0.963119  \n",
       "6             0.017294             0.989754  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Overlap. In some cases, you may want to merge classes in the top rows (below)\n",
      "-----------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Name A</th>\n",
       "      <th>Class Name B</th>\n",
       "      <th>Class Index A</th>\n",
       "      <th>Class Index B</th>\n",
       "      <th>Num Overlapping Examples</th>\n",
       "      <th>Joint Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT과학</td>\n",
       "      <td>경제</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>185</td>\n",
       "      <td>0.010925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>사회</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>127</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>사회</td>\n",
       "      <td>정치</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>0.005551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>경제</td>\n",
       "      <td>사회</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>0.005433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IT과학</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>0.004075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IT과학</td>\n",
       "      <td>세계</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>0.003957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>경제</td>\n",
       "      <td>세계</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.003780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>경제</td>\n",
       "      <td>생활문화</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>0.003721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IT과학</td>\n",
       "      <td>사회</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>0.003012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>세계</td>\n",
       "      <td>정치</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>0.002894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>사회</td>\n",
       "      <td>세계</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>생활문화</td>\n",
       "      <td>세계</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0.001654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>생활문화</td>\n",
       "      <td>정치</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>0.001063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>경제</td>\n",
       "      <td>정치</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>세계</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>사회</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IT과학</td>\n",
       "      <td>정치</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>생활문화</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>스포츠</td>\n",
       "      <td>정치</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>경제</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>IT과학</td>\n",
       "      <td>스포츠</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Name A Class Name B  Class Index A  Class Index B   \n",
       "0          IT과학           경제              0              1  \\\n",
       "1            사회         생활문화              2              3   \n",
       "2            사회           정치              2              6   \n",
       "3            경제           사회              1              2   \n",
       "4          IT과학         생활문화              0              3   \n",
       "5          IT과학           세계              0              4   \n",
       "6            경제           세계              1              4   \n",
       "7            경제         생활문화              1              3   \n",
       "8          IT과학           사회              0              2   \n",
       "9            세계           정치              4              6   \n",
       "10           사회           세계              2              4   \n",
       "11         생활문화           세계              3              4   \n",
       "12         생활문화           정치              3              6   \n",
       "13           경제           정치              1              6   \n",
       "14           세계          스포츠              4              5   \n",
       "15           사회          스포츠              2              5   \n",
       "16         IT과학           정치              0              6   \n",
       "17         생활문화          스포츠              3              5   \n",
       "18          스포츠           정치              5              6   \n",
       "19           경제          스포츠              1              5   \n",
       "20         IT과학          스포츠              0              5   \n",
       "\n",
       "    Num Overlapping Examples  Joint Probability  \n",
       "0                        185           0.010925  \n",
       "1                        127           0.007500  \n",
       "2                         94           0.005551  \n",
       "3                         92           0.005433  \n",
       "4                         69           0.004075  \n",
       "5                         67           0.003957  \n",
       "6                         64           0.003780  \n",
       "7                         63           0.003721  \n",
       "8                         51           0.003012  \n",
       "9                         49           0.002894  \n",
       "10                        32           0.001890  \n",
       "11                        28           0.001654  \n",
       "12                        18           0.001063  \n",
       "13                        13           0.000768  \n",
       "14                         9           0.000532  \n",
       "15                         8           0.000472  \n",
       "16                         8           0.000472  \n",
       "17                         4           0.000236  \n",
       "18                         3           0.000177  \n",
       "19                         2           0.000118  \n",
       "20                         1           0.000059  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * Overall, about 6% (987 of the 16,933) labels in your dataset have potential issues.\n",
      " ** The overall label health score for this dataset is: 0.94.\n",
      "\n",
      "Generated with <3 from Cleanlab.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_label_health_score': 0.9417114510128153,\n",
       " 'joint': array([[1.35829445e-01, 4.31110849e-03, 1.41735074e-03, 2.12602610e-03,\n",
       "         1.65357586e-03, 5.90562806e-05, 1.77168842e-04],\n",
       "        [6.61430343e-03, 1.48290321e-01, 2.42130751e-03, 1.41735074e-03,\n",
       "         2.30319494e-03, 5.90562806e-05, 4.72450245e-04],\n",
       "        [1.59451958e-03, 3.01187031e-03, 1.48467490e-01, 4.78355873e-03,\n",
       "         1.35829445e-03, 2.95281403e-04, 2.77564519e-03],\n",
       "        [1.94885726e-03, 2.30319494e-03, 2.71658891e-03, 1.34648320e-01,\n",
       "         1.06301305e-03, 1.18112561e-04, 4.13393964e-04],\n",
       "        [2.30319494e-03, 1.47640702e-03, 5.31506526e-04, 5.90562806e-04,\n",
       "         1.61932322e-01, 3.54337684e-04, 9.44900490e-04],\n",
       "        [0.00000000e+00, 5.90562806e-05, 1.77168842e-04, 1.18112561e-04,\n",
       "         1.77168842e-04, 5.70483671e-02, 5.90562806e-05],\n",
       "        [2.95281403e-04, 2.95281403e-04, 2.77564519e-03, 6.49619087e-04,\n",
       "         1.94885726e-03, 1.18112561e-04, 1.55495187e-01]]),\n",
       " 'classes_by_label_quality':   Class Name  Class Index  Label Issues  Inverse Label Issues  Label Noise   \n",
       " 0         사회            2           234                   170     0.085153  \\\n",
       " 1         경제            1           225                   194     0.082237   \n",
       " 2       IT과학            0           165                   216     0.066937   \n",
       " 3       생활문화            3           145                   164     0.059794   \n",
       " 4         정치            6           103                    82     0.037646   \n",
       " 5         세계            4           105                   144     0.036881   \n",
       " 6        스포츠            5            10                    17     0.010246   \n",
       " \n",
       "    Inverse Label Noise  Label Quality Score  \n",
       " 0             0.063338             0.914847  \n",
       " 1             0.071719             0.917763  \n",
       " 2             0.085851             0.933063  \n",
       " 3             0.067103             0.940206  \n",
       " 4             0.030203             0.962354  \n",
       " 5             0.049896             0.963119  \n",
       " 6             0.017294             0.989754  ,\n",
       " 'overlapping_classes':    Class Name A Class Name B  Class Index A  Class Index B   \n",
       " 0          IT과학           경제              0              1  \\\n",
       " 1            사회         생활문화              2              3   \n",
       " 2            사회           정치              2              6   \n",
       " 3            경제           사회              1              2   \n",
       " 4          IT과학         생활문화              0              3   \n",
       " 5          IT과학           세계              0              4   \n",
       " 6            경제           세계              1              4   \n",
       " 7            경제         생활문화              1              3   \n",
       " 8          IT과학           사회              0              2   \n",
       " 9            세계           정치              4              6   \n",
       " 10           사회           세계              2              4   \n",
       " 11         생활문화           세계              3              4   \n",
       " 12         생활문화           정치              3              6   \n",
       " 13           경제           정치              1              6   \n",
       " 14           세계          스포츠              4              5   \n",
       " 15           사회          스포츠              2              5   \n",
       " 16         IT과학           정치              0              6   \n",
       " 17         생활문화          스포츠              3              5   \n",
       " 18          스포츠           정치              5              6   \n",
       " 19           경제          스포츠              1              5   \n",
       " 20         IT과학          스포츠              0              5   \n",
       " \n",
       "     Num Overlapping Examples  Joint Probability  \n",
       " 0                        185           0.010925  \n",
       " 1                        127           0.007500  \n",
       " 2                         94           0.005551  \n",
       " 3                         92           0.005433  \n",
       " 4                         69           0.004075  \n",
       " 5                         67           0.003957  \n",
       " 6                         64           0.003780  \n",
       " 7                         63           0.003721  \n",
       " 8                         51           0.003012  \n",
       " 9                         49           0.002894  \n",
       " 10                        32           0.001890  \n",
       " 11                        28           0.001654  \n",
       " 12                        18           0.001063  \n",
       " 13                        13           0.000768  \n",
       " 14                         9           0.000532  \n",
       " 15                         8           0.000472  \n",
       " 16                         8           0.000472  \n",
       " 17                         4           0.000236  \n",
       " 18                         3           0.000177  \n",
       " 19                         2           0.000118  \n",
       " 20                         1           0.000059  }"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cleanlab.dataset import health_summary\n",
    "class_names=[0,1,2,3,4,5,6]\n",
    "health_summary(data['target'], pred_probs_array, class_names=[num2topic[i] for i in class_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16933\n",
      "15946\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>겨울에도 수영장이 따뜻하네3400억 투자금 유치 컴퓨터로 데웠다테크토크</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5G 28 주파수 값 2000억 육박예상보다 많이 올랐다종합</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>컴퓨터비전 분야 AI 인간보다 경제성 떨어진다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>獨 철도 30년 된 MSDOS윈도 3.11 관리할 경력자 찾아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>인류 첫 지구 아닌 행성 비행 화성탐사 인저뉴이티 역사속으로</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15941</th>\n",
       "      <td>힐만 SK 감독 고통스럽지만 내 상황 솔직히 알려야 해</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15942</th>\n",
       "      <td>정의장 사드 국회동의 사안 아니라 쳐도 충분히 협의해야</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15943</th>\n",
       "      <td>정치권 엘시티 수사 돌발변수에 촉각…왜 지금</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15944</th>\n",
       "      <td>문 대통령 1987 관람…깜짝 방문에 객석 환호·박수종합</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15945</th>\n",
       "      <td>120년 전 대한제국으로…가을밤 정동에서 시간 여행 떠나다</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15946 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text  target\n",
       "0      겨울에도 수영장이 따뜻하네3400억 투자금 유치 컴퓨터로 데웠다테크토크       0\n",
       "1            5G 28 주파수 값 2000억 육박예상보다 많이 올랐다종합       0\n",
       "2                    컴퓨터비전 분야 AI 인간보다 경제성 떨어진다       0\n",
       "3           獨 철도 30년 된 MSDOS윈도 3.11 관리할 경력자 찾아       0\n",
       "4            인류 첫 지구 아닌 행성 비행 화성탐사 인저뉴이티 역사속으로       0\n",
       "...                                        ...     ...\n",
       "15941           힐만 SK 감독 고통스럽지만 내 상황 솔직히 알려야 해       5\n",
       "15942           정의장 사드 국회동의 사안 아니라 쳐도 충분히 협의해야       6\n",
       "15943                 정치권 엘시티 수사 돌발변수에 촉각…왜 지금       6\n",
       "15944          문 대통령 1987 관람…깜짝 방문에 객석 환호·박수종합       6\n",
       "15945         120년 전 대한제국으로…가을밤 정동에서 시간 여행 떠나다       3\n",
       "\n",
       "[15946 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = data.drop(index=list(ordered_label_issues)).reset_index(drop=True)\n",
    "print(len(data))\n",
    "print(len(cleaned_data))\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv(\"cleaned_data.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
